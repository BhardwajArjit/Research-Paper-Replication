{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOg7LdbYVMrqxfphXGoMKIw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhardwajArjit/Research-Paper-Replication/blob/main/ResNet_Replication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This notebook replicates the research paper titled \"**Deep Residual Learning for Image Recognition**\" with PyTorch.\n",
        "\n",
        "The link to the paper: https://arxiv.org/abs/1512.03385\n",
        "\n",
        "ResNet (Residual Network) is a deep neural network architecture that uses skip connections to facilitate training of very deep convolutional neural networks."
      ],
      "metadata": {
        "id": "9Rlre1T3Sv50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Get setup"
      ],
      "metadata": {
        "id": "R8g_uICpesBS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_1P56cgfXfJX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# regular imports\n",
        "import os\n",
        "import shutil\n",
        "from collections import OrderedDict"
      ],
      "metadata": {
        "id": "zU1QR88PfRFg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOOg10nwfUAG",
        "outputId": "507997f6-6a92-4980-f9fd-c79ce8945f7d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU provided by Google Colab\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_PvandVffea",
        "outputId": "0d265874-5b34-43d5-f9d7-1845def75c86"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Nov 18 10:34:13 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8    10W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Get data"
      ],
      "metadata": {
        "id": "AiQJlzha7O2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataloader_cifar():\n",
        "    transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mean=[0.5], std=[0.5])])\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(root=\"data\", train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.CIFAR10(root=\"data\", train=False, download=True, transform=transform)\n",
        "\n",
        "    # Split dataset into training set and validation set.\n",
        "    train_dataset, val_dataset = random_split(train_dataset, (45000, 5000))\n",
        "\n",
        "    print(\"Image shape of a random sample image : {}\".format(train_dataset[0][0].numpy().shape), end = '\\n\\n')\n",
        "\n",
        "    print(\"Training Set:   {} images\".format(len(train_dataset)))\n",
        "    print(\"Validation Set:   {} images\".format(len(val_dataset)))\n",
        "    print(\"Test Set:       {} images\".format(len(test_dataset)))\n",
        "\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    # Generate dataloader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=10000, shuffle=True)\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "A_PKurfphIBq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, val_loader, test_loader = dataloader_cifar()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiQT3tsGh4yJ",
        "outputId": "82706345-69ae-4784-f408-37349116f757"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:11<00:00, 15035586.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n",
            "Image shape of a random sample image : (3, 32, 32)\n",
            "\n",
            "Training Set:   45000 images\n",
            "Validation Set:   5000 images\n",
            "Test Set:       10000 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. ResNet implementation"
      ],
      "metadata": {
        "id": "iDVUjjvw73mN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class block(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels, intermediate_channels, identity_downsample=None, stride=1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.expansion = 4\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels,\n",
        "            intermediate_channels,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(intermediate_channels)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            intermediate_channels,\n",
        "            intermediate_channels,\n",
        "            kernel_size=3,\n",
        "            stride=stride,\n",
        "            padding=1,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(intermediate_channels)\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            intermediate_channels,\n",
        "            intermediate_channels * self.expansion,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.bn3 = nn.BatchNorm2d(intermediate_channels * self.expansion)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.identity_downsample = identity_downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "\n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(identity)\n",
        "\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "iKxlDxsuouYA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, image_channels, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            image_channels, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Essentially the entire ResNet architecture are in these 4 lines below\n",
        "        self.layer1 = self._make_layer(\n",
        "            block, layers[0], intermediate_channels=64, stride=1\n",
        "        )\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, layers[1], intermediate_channels=128, stride=2\n",
        "        )\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, layers[2], intermediate_channels=256, stride=2\n",
        "        )\n",
        "        self.layer4 = self._make_layer(\n",
        "            block, layers[3], intermediate_channels=512, stride=2\n",
        "        )\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * 4, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _make_layer(self, block, num_residual_blocks, intermediate_channels, stride):\n",
        "        identity_downsample = None\n",
        "        layers = []\n",
        "\n",
        "        if stride != 1 or self.in_channels != intermediate_channels * 4:\n",
        "            identity_downsample = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    self.in_channels,\n",
        "                    intermediate_channels * 4,\n",
        "                    kernel_size=1,\n",
        "                    stride=stride,\n",
        "                    bias=False,\n",
        "                ),\n",
        "                nn.BatchNorm2d(intermediate_channels * 4),\n",
        "            )\n",
        "\n",
        "        layers.append(\n",
        "            block(self.in_channels, intermediate_channels, identity_downsample, stride)\n",
        "        )\n",
        "\n",
        "        # The expansion size is always 4 for ResNet 50,101,152\n",
        "        self.in_channels = intermediate_channels * 4\n",
        "\n",
        "\n",
        "        for i in range(num_residual_blocks - 1):\n",
        "            layers.append(block(self.in_channels, intermediate_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "4trodm5slrB5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ResNet18(img_channel=3, num_classes=1000):\n",
        "  return ResNet(block, [2, 2, 2, 2], img_channel, num_classes)\n",
        "\n",
        "\n",
        "def ResNet50(img_channel=3, num_classes=1000):\n",
        "    return ResNet(block, [3, 4, 6, 3], img_channel, num_classes)\n",
        "\n",
        "\n",
        "def ResNet101(img_channel=3, num_classes=1000):\n",
        "    return ResNet(block, [3, 4, 23, 3], img_channel, num_classes)\n",
        "\n",
        "\n",
        "def ResNet152(img_channel=3, num_classes=1000):\n",
        "    return ResNet(block, [3, 8, 36, 3], img_channel, num_classes)\n",
        "\n",
        "def test():\n",
        "    BATCH_SIZE = 4\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    net = ResNet18(img_channel=3, num_classes=1000).to(device)\n",
        "\n",
        "    # Create a random input tensor and move it to the same device\n",
        "    input_tensor = torch.randn(BATCH_SIZE, 3, 224, 224).to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    y = net(input_tensor)\n",
        "\n",
        "    assert y.size() == torch.Size([BATCH_SIZE, 1000])\n",
        "    print(y.size())"
      ],
      "metadata": {
        "id": "YLXro-PCxaa8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test()"
      ],
      "metadata": {
        "id": "S85ZpAz5vkuJ",
        "outputId": "964756b8-7eea-4974-e04e-76a4cf8227d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet18 = ResNet18()\n",
        "summary(resnet18.to(device), (3, 32, 32))"
      ],
      "metadata": {
        "id": "1Y-RWZ0JvrvX",
        "outputId": "67c39925-7b80-4e9f-bfe7-969fded7b1ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "              ReLU-3           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
            "            Conv2d-5             [-1, 64, 8, 8]           4,096\n",
            "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
            "              ReLU-7             [-1, 64, 8, 8]               0\n",
            "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
            "             ReLU-10             [-1, 64, 8, 8]               0\n",
            "           Conv2d-11            [-1, 256, 8, 8]          16,384\n",
            "      BatchNorm2d-12            [-1, 256, 8, 8]             512\n",
            "           Conv2d-13            [-1, 256, 8, 8]          16,384\n",
            "      BatchNorm2d-14            [-1, 256, 8, 8]             512\n",
            "             ReLU-15            [-1, 256, 8, 8]               0\n",
            "            block-16            [-1, 256, 8, 8]               0\n",
            "           Conv2d-17             [-1, 64, 8, 8]          16,384\n",
            "      BatchNorm2d-18             [-1, 64, 8, 8]             128\n",
            "             ReLU-19             [-1, 64, 8, 8]               0\n",
            "           Conv2d-20             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-21             [-1, 64, 8, 8]             128\n",
            "             ReLU-22             [-1, 64, 8, 8]               0\n",
            "           Conv2d-23            [-1, 256, 8, 8]          16,384\n",
            "      BatchNorm2d-24            [-1, 256, 8, 8]             512\n",
            "             ReLU-25            [-1, 256, 8, 8]               0\n",
            "            block-26            [-1, 256, 8, 8]               0\n",
            "           Conv2d-27            [-1, 128, 8, 8]          32,768\n",
            "      BatchNorm2d-28            [-1, 128, 8, 8]             256\n",
            "             ReLU-29            [-1, 128, 8, 8]               0\n",
            "           Conv2d-30            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-31            [-1, 128, 4, 4]             256\n",
            "             ReLU-32            [-1, 128, 4, 4]               0\n",
            "           Conv2d-33            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-34            [-1, 512, 4, 4]           1,024\n",
            "           Conv2d-35            [-1, 512, 4, 4]         131,072\n",
            "      BatchNorm2d-36            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-37            [-1, 512, 4, 4]               0\n",
            "            block-38            [-1, 512, 4, 4]               0\n",
            "           Conv2d-39            [-1, 128, 4, 4]          65,536\n",
            "      BatchNorm2d-40            [-1, 128, 4, 4]             256\n",
            "             ReLU-41            [-1, 128, 4, 4]               0\n",
            "           Conv2d-42            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-43            [-1, 128, 4, 4]             256\n",
            "             ReLU-44            [-1, 128, 4, 4]               0\n",
            "           Conv2d-45            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-46            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-47            [-1, 512, 4, 4]               0\n",
            "            block-48            [-1, 512, 4, 4]               0\n",
            "           Conv2d-49            [-1, 256, 4, 4]         131,072\n",
            "      BatchNorm2d-50            [-1, 256, 4, 4]             512\n",
            "             ReLU-51            [-1, 256, 4, 4]               0\n",
            "           Conv2d-52            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-53            [-1, 256, 2, 2]             512\n",
            "             ReLU-54            [-1, 256, 2, 2]               0\n",
            "           Conv2d-55           [-1, 1024, 2, 2]         262,144\n",
            "      BatchNorm2d-56           [-1, 1024, 2, 2]           2,048\n",
            "           Conv2d-57           [-1, 1024, 2, 2]         524,288\n",
            "      BatchNorm2d-58           [-1, 1024, 2, 2]           2,048\n",
            "             ReLU-59           [-1, 1024, 2, 2]               0\n",
            "            block-60           [-1, 1024, 2, 2]               0\n",
            "           Conv2d-61            [-1, 256, 2, 2]         262,144\n",
            "      BatchNorm2d-62            [-1, 256, 2, 2]             512\n",
            "             ReLU-63            [-1, 256, 2, 2]               0\n",
            "           Conv2d-64            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-65            [-1, 256, 2, 2]             512\n",
            "             ReLU-66            [-1, 256, 2, 2]               0\n",
            "           Conv2d-67           [-1, 1024, 2, 2]         262,144\n",
            "      BatchNorm2d-68           [-1, 1024, 2, 2]           2,048\n",
            "             ReLU-69           [-1, 1024, 2, 2]               0\n",
            "            block-70           [-1, 1024, 2, 2]               0\n",
            "           Conv2d-71            [-1, 512, 2, 2]         524,288\n",
            "      BatchNorm2d-72            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-73            [-1, 512, 2, 2]               0\n",
            "           Conv2d-74            [-1, 512, 1, 1]       2,359,296\n",
            "      BatchNorm2d-75            [-1, 512, 1, 1]           1,024\n",
            "             ReLU-76            [-1, 512, 1, 1]               0\n",
            "           Conv2d-77           [-1, 2048, 1, 1]       1,048,576\n",
            "      BatchNorm2d-78           [-1, 2048, 1, 1]           4,096\n",
            "           Conv2d-79           [-1, 2048, 1, 1]       2,097,152\n",
            "      BatchNorm2d-80           [-1, 2048, 1, 1]           4,096\n",
            "             ReLU-81           [-1, 2048, 1, 1]               0\n",
            "            block-82           [-1, 2048, 1, 1]               0\n",
            "           Conv2d-83            [-1, 512, 1, 1]       1,048,576\n",
            "      BatchNorm2d-84            [-1, 512, 1, 1]           1,024\n",
            "             ReLU-85            [-1, 512, 1, 1]               0\n",
            "           Conv2d-86            [-1, 512, 1, 1]       2,359,296\n",
            "      BatchNorm2d-87            [-1, 512, 1, 1]           1,024\n",
            "             ReLU-88            [-1, 512, 1, 1]               0\n",
            "           Conv2d-89           [-1, 2048, 1, 1]       1,048,576\n",
            "      BatchNorm2d-90           [-1, 2048, 1, 1]           4,096\n",
            "             ReLU-91           [-1, 2048, 1, 1]               0\n",
            "            block-92           [-1, 2048, 1, 1]               0\n",
            "AdaptiveAvgPool2d-93           [-1, 2048, 1, 1]               0\n",
            "           Linear-94                 [-1, 1000]       2,049,000\n",
            "================================================================\n",
            "Total params: 15,995,176\n",
            "Trainable params: 15,995,176\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 3.72\n",
            "Params size (MB): 61.02\n",
            "Estimated Total Size (MB): 64.75\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet152 = ResNet152()\n",
        "summary(resnet152.to(device), (3, 32, 32))"
      ],
      "metadata": {
        "id": "tHaBb3NQzHWD",
        "outputId": "16254b14-3bf1-4ebe-d080-e0781a4e553a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "              ReLU-3           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
            "            Conv2d-5             [-1, 64, 8, 8]           4,096\n",
            "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
            "              ReLU-7             [-1, 64, 8, 8]               0\n",
            "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
            "             ReLU-10             [-1, 64, 8, 8]               0\n",
            "           Conv2d-11            [-1, 256, 8, 8]          16,384\n",
            "      BatchNorm2d-12            [-1, 256, 8, 8]             512\n",
            "           Conv2d-13            [-1, 256, 8, 8]          16,384\n",
            "      BatchNorm2d-14            [-1, 256, 8, 8]             512\n",
            "             ReLU-15            [-1, 256, 8, 8]               0\n",
            "            block-16            [-1, 256, 8, 8]               0\n",
            "           Conv2d-17             [-1, 64, 8, 8]          16,384\n",
            "      BatchNorm2d-18             [-1, 64, 8, 8]             128\n",
            "             ReLU-19             [-1, 64, 8, 8]               0\n",
            "           Conv2d-20             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-21             [-1, 64, 8, 8]             128\n",
            "             ReLU-22             [-1, 64, 8, 8]               0\n",
            "           Conv2d-23            [-1, 256, 8, 8]          16,384\n",
            "      BatchNorm2d-24            [-1, 256, 8, 8]             512\n",
            "             ReLU-25            [-1, 256, 8, 8]               0\n",
            "            block-26            [-1, 256, 8, 8]               0\n",
            "           Conv2d-27             [-1, 64, 8, 8]          16,384\n",
            "      BatchNorm2d-28             [-1, 64, 8, 8]             128\n",
            "             ReLU-29             [-1, 64, 8, 8]               0\n",
            "           Conv2d-30             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-31             [-1, 64, 8, 8]             128\n",
            "             ReLU-32             [-1, 64, 8, 8]               0\n",
            "           Conv2d-33            [-1, 256, 8, 8]          16,384\n",
            "      BatchNorm2d-34            [-1, 256, 8, 8]             512\n",
            "             ReLU-35            [-1, 256, 8, 8]               0\n",
            "            block-36            [-1, 256, 8, 8]               0\n",
            "           Conv2d-37            [-1, 128, 8, 8]          32,768\n",
            "      BatchNorm2d-38            [-1, 128, 8, 8]             256\n",
            "             ReLU-39            [-1, 128, 8, 8]               0\n",
            "           Conv2d-40            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-41            [-1, 128, 4, 4]             256\n",
            "             ReLU-42            [-1, 128, 4, 4]               0\n",
            "           Conv2d-43            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-44            [-1, 512, 4, 4]           1,024\n",
            "           Conv2d-45            [-1, 512, 4, 4]         131,072\n",
            "      BatchNorm2d-46            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-47            [-1, 512, 4, 4]               0\n",
            "            block-48            [-1, 512, 4, 4]               0\n",
            "           Conv2d-49            [-1, 128, 4, 4]          65,536\n",
            "      BatchNorm2d-50            [-1, 128, 4, 4]             256\n",
            "             ReLU-51            [-1, 128, 4, 4]               0\n",
            "           Conv2d-52            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-53            [-1, 128, 4, 4]             256\n",
            "             ReLU-54            [-1, 128, 4, 4]               0\n",
            "           Conv2d-55            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-56            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-57            [-1, 512, 4, 4]               0\n",
            "            block-58            [-1, 512, 4, 4]               0\n",
            "           Conv2d-59            [-1, 128, 4, 4]          65,536\n",
            "      BatchNorm2d-60            [-1, 128, 4, 4]             256\n",
            "             ReLU-61            [-1, 128, 4, 4]               0\n",
            "           Conv2d-62            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-63            [-1, 128, 4, 4]             256\n",
            "             ReLU-64            [-1, 128, 4, 4]               0\n",
            "           Conv2d-65            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-66            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-67            [-1, 512, 4, 4]               0\n",
            "            block-68            [-1, 512, 4, 4]               0\n",
            "           Conv2d-69            [-1, 128, 4, 4]          65,536\n",
            "      BatchNorm2d-70            [-1, 128, 4, 4]             256\n",
            "             ReLU-71            [-1, 128, 4, 4]               0\n",
            "           Conv2d-72            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-73            [-1, 128, 4, 4]             256\n",
            "             ReLU-74            [-1, 128, 4, 4]               0\n",
            "           Conv2d-75            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-76            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-77            [-1, 512, 4, 4]               0\n",
            "            block-78            [-1, 512, 4, 4]               0\n",
            "           Conv2d-79            [-1, 128, 4, 4]          65,536\n",
            "      BatchNorm2d-80            [-1, 128, 4, 4]             256\n",
            "             ReLU-81            [-1, 128, 4, 4]               0\n",
            "           Conv2d-82            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-83            [-1, 128, 4, 4]             256\n",
            "             ReLU-84            [-1, 128, 4, 4]               0\n",
            "           Conv2d-85            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-86            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-87            [-1, 512, 4, 4]               0\n",
            "            block-88            [-1, 512, 4, 4]               0\n",
            "           Conv2d-89            [-1, 128, 4, 4]          65,536\n",
            "      BatchNorm2d-90            [-1, 128, 4, 4]             256\n",
            "             ReLU-91            [-1, 128, 4, 4]               0\n",
            "           Conv2d-92            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-93            [-1, 128, 4, 4]             256\n",
            "             ReLU-94            [-1, 128, 4, 4]               0\n",
            "           Conv2d-95            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-96            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-97            [-1, 512, 4, 4]               0\n",
            "            block-98            [-1, 512, 4, 4]               0\n",
            "           Conv2d-99            [-1, 128, 4, 4]          65,536\n",
            "     BatchNorm2d-100            [-1, 128, 4, 4]             256\n",
            "            ReLU-101            [-1, 128, 4, 4]               0\n",
            "          Conv2d-102            [-1, 128, 4, 4]         147,456\n",
            "     BatchNorm2d-103            [-1, 128, 4, 4]             256\n",
            "            ReLU-104            [-1, 128, 4, 4]               0\n",
            "          Conv2d-105            [-1, 512, 4, 4]          65,536\n",
            "     BatchNorm2d-106            [-1, 512, 4, 4]           1,024\n",
            "            ReLU-107            [-1, 512, 4, 4]               0\n",
            "           block-108            [-1, 512, 4, 4]               0\n",
            "          Conv2d-109            [-1, 128, 4, 4]          65,536\n",
            "     BatchNorm2d-110            [-1, 128, 4, 4]             256\n",
            "            ReLU-111            [-1, 128, 4, 4]               0\n",
            "          Conv2d-112            [-1, 128, 4, 4]         147,456\n",
            "     BatchNorm2d-113            [-1, 128, 4, 4]             256\n",
            "            ReLU-114            [-1, 128, 4, 4]               0\n",
            "          Conv2d-115            [-1, 512, 4, 4]          65,536\n",
            "     BatchNorm2d-116            [-1, 512, 4, 4]           1,024\n",
            "            ReLU-117            [-1, 512, 4, 4]               0\n",
            "           block-118            [-1, 512, 4, 4]               0\n",
            "          Conv2d-119            [-1, 256, 4, 4]         131,072\n",
            "     BatchNorm2d-120            [-1, 256, 4, 4]             512\n",
            "            ReLU-121            [-1, 256, 4, 4]               0\n",
            "          Conv2d-122            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-123            [-1, 256, 2, 2]             512\n",
            "            ReLU-124            [-1, 256, 2, 2]               0\n",
            "          Conv2d-125           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-126           [-1, 1024, 2, 2]           2,048\n",
            "          Conv2d-127           [-1, 1024, 2, 2]         524,288\n",
            "     BatchNorm2d-128           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-129           [-1, 1024, 2, 2]               0\n",
            "           block-130           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-131            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-132            [-1, 256, 2, 2]             512\n",
            "            ReLU-133            [-1, 256, 2, 2]               0\n",
            "          Conv2d-134            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-135            [-1, 256, 2, 2]             512\n",
            "            ReLU-136            [-1, 256, 2, 2]               0\n",
            "          Conv2d-137           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-138           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-139           [-1, 1024, 2, 2]               0\n",
            "           block-140           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-141            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-142            [-1, 256, 2, 2]             512\n",
            "            ReLU-143            [-1, 256, 2, 2]               0\n",
            "          Conv2d-144            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-145            [-1, 256, 2, 2]             512\n",
            "            ReLU-146            [-1, 256, 2, 2]               0\n",
            "          Conv2d-147           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-148           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-149           [-1, 1024, 2, 2]               0\n",
            "           block-150           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-151            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-152            [-1, 256, 2, 2]             512\n",
            "            ReLU-153            [-1, 256, 2, 2]               0\n",
            "          Conv2d-154            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-155            [-1, 256, 2, 2]             512\n",
            "            ReLU-156            [-1, 256, 2, 2]               0\n",
            "          Conv2d-157           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-158           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-159           [-1, 1024, 2, 2]               0\n",
            "           block-160           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-161            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-162            [-1, 256, 2, 2]             512\n",
            "            ReLU-163            [-1, 256, 2, 2]               0\n",
            "          Conv2d-164            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-165            [-1, 256, 2, 2]             512\n",
            "            ReLU-166            [-1, 256, 2, 2]               0\n",
            "          Conv2d-167           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-168           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-169           [-1, 1024, 2, 2]               0\n",
            "           block-170           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-171            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-172            [-1, 256, 2, 2]             512\n",
            "            ReLU-173            [-1, 256, 2, 2]               0\n",
            "          Conv2d-174            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-175            [-1, 256, 2, 2]             512\n",
            "            ReLU-176            [-1, 256, 2, 2]               0\n",
            "          Conv2d-177           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-178           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-179           [-1, 1024, 2, 2]               0\n",
            "           block-180           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-181            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-182            [-1, 256, 2, 2]             512\n",
            "            ReLU-183            [-1, 256, 2, 2]               0\n",
            "          Conv2d-184            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-185            [-1, 256, 2, 2]             512\n",
            "            ReLU-186            [-1, 256, 2, 2]               0\n",
            "          Conv2d-187           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-188           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-189           [-1, 1024, 2, 2]               0\n",
            "           block-190           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-191            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-192            [-1, 256, 2, 2]             512\n",
            "            ReLU-193            [-1, 256, 2, 2]               0\n",
            "          Conv2d-194            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-195            [-1, 256, 2, 2]             512\n",
            "            ReLU-196            [-1, 256, 2, 2]               0\n",
            "          Conv2d-197           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-198           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-199           [-1, 1024, 2, 2]               0\n",
            "           block-200           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-201            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-202            [-1, 256, 2, 2]             512\n",
            "            ReLU-203            [-1, 256, 2, 2]               0\n",
            "          Conv2d-204            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-205            [-1, 256, 2, 2]             512\n",
            "            ReLU-206            [-1, 256, 2, 2]               0\n",
            "          Conv2d-207           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-208           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-209           [-1, 1024, 2, 2]               0\n",
            "           block-210           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-211            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-212            [-1, 256, 2, 2]             512\n",
            "            ReLU-213            [-1, 256, 2, 2]               0\n",
            "          Conv2d-214            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-215            [-1, 256, 2, 2]             512\n",
            "            ReLU-216            [-1, 256, 2, 2]               0\n",
            "          Conv2d-217           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-218           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-219           [-1, 1024, 2, 2]               0\n",
            "           block-220           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-221            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-222            [-1, 256, 2, 2]             512\n",
            "            ReLU-223            [-1, 256, 2, 2]               0\n",
            "          Conv2d-224            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-225            [-1, 256, 2, 2]             512\n",
            "            ReLU-226            [-1, 256, 2, 2]               0\n",
            "          Conv2d-227           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-228           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-229           [-1, 1024, 2, 2]               0\n",
            "           block-230           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-231            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-232            [-1, 256, 2, 2]             512\n",
            "            ReLU-233            [-1, 256, 2, 2]               0\n",
            "          Conv2d-234            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-235            [-1, 256, 2, 2]             512\n",
            "            ReLU-236            [-1, 256, 2, 2]               0\n",
            "          Conv2d-237           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-238           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-239           [-1, 1024, 2, 2]               0\n",
            "           block-240           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-241            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-242            [-1, 256, 2, 2]             512\n",
            "            ReLU-243            [-1, 256, 2, 2]               0\n",
            "          Conv2d-244            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-245            [-1, 256, 2, 2]             512\n",
            "            ReLU-246            [-1, 256, 2, 2]               0\n",
            "          Conv2d-247           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-248           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-249           [-1, 1024, 2, 2]               0\n",
            "           block-250           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-251            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-252            [-1, 256, 2, 2]             512\n",
            "            ReLU-253            [-1, 256, 2, 2]               0\n",
            "          Conv2d-254            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-255            [-1, 256, 2, 2]             512\n",
            "            ReLU-256            [-1, 256, 2, 2]               0\n",
            "          Conv2d-257           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-258           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-259           [-1, 1024, 2, 2]               0\n",
            "           block-260           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-261            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-262            [-1, 256, 2, 2]             512\n",
            "            ReLU-263            [-1, 256, 2, 2]               0\n",
            "          Conv2d-264            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-265            [-1, 256, 2, 2]             512\n",
            "            ReLU-266            [-1, 256, 2, 2]               0\n",
            "          Conv2d-267           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-268           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-269           [-1, 1024, 2, 2]               0\n",
            "           block-270           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-271            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-272            [-1, 256, 2, 2]             512\n",
            "            ReLU-273            [-1, 256, 2, 2]               0\n",
            "          Conv2d-274            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-275            [-1, 256, 2, 2]             512\n",
            "            ReLU-276            [-1, 256, 2, 2]               0\n",
            "          Conv2d-277           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-278           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-279           [-1, 1024, 2, 2]               0\n",
            "           block-280           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-281            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-282            [-1, 256, 2, 2]             512\n",
            "            ReLU-283            [-1, 256, 2, 2]               0\n",
            "          Conv2d-284            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-285            [-1, 256, 2, 2]             512\n",
            "            ReLU-286            [-1, 256, 2, 2]               0\n",
            "          Conv2d-287           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-288           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-289           [-1, 1024, 2, 2]               0\n",
            "           block-290           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-291            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-292            [-1, 256, 2, 2]             512\n",
            "            ReLU-293            [-1, 256, 2, 2]               0\n",
            "          Conv2d-294            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-295            [-1, 256, 2, 2]             512\n",
            "            ReLU-296            [-1, 256, 2, 2]               0\n",
            "          Conv2d-297           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-298           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-299           [-1, 1024, 2, 2]               0\n",
            "           block-300           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-301            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-302            [-1, 256, 2, 2]             512\n",
            "            ReLU-303            [-1, 256, 2, 2]               0\n",
            "          Conv2d-304            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-305            [-1, 256, 2, 2]             512\n",
            "            ReLU-306            [-1, 256, 2, 2]               0\n",
            "          Conv2d-307           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-308           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-309           [-1, 1024, 2, 2]               0\n",
            "           block-310           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-311            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-312            [-1, 256, 2, 2]             512\n",
            "            ReLU-313            [-1, 256, 2, 2]               0\n",
            "          Conv2d-314            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-315            [-1, 256, 2, 2]             512\n",
            "            ReLU-316            [-1, 256, 2, 2]               0\n",
            "          Conv2d-317           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-318           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-319           [-1, 1024, 2, 2]               0\n",
            "           block-320           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-321            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-322            [-1, 256, 2, 2]             512\n",
            "            ReLU-323            [-1, 256, 2, 2]               0\n",
            "          Conv2d-324            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-325            [-1, 256, 2, 2]             512\n",
            "            ReLU-326            [-1, 256, 2, 2]               0\n",
            "          Conv2d-327           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-328           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-329           [-1, 1024, 2, 2]               0\n",
            "           block-330           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-331            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-332            [-1, 256, 2, 2]             512\n",
            "            ReLU-333            [-1, 256, 2, 2]               0\n",
            "          Conv2d-334            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-335            [-1, 256, 2, 2]             512\n",
            "            ReLU-336            [-1, 256, 2, 2]               0\n",
            "          Conv2d-337           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-338           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-339           [-1, 1024, 2, 2]               0\n",
            "           block-340           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-341            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-342            [-1, 256, 2, 2]             512\n",
            "            ReLU-343            [-1, 256, 2, 2]               0\n",
            "          Conv2d-344            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-345            [-1, 256, 2, 2]             512\n",
            "            ReLU-346            [-1, 256, 2, 2]               0\n",
            "          Conv2d-347           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-348           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-349           [-1, 1024, 2, 2]               0\n",
            "           block-350           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-351            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-352            [-1, 256, 2, 2]             512\n",
            "            ReLU-353            [-1, 256, 2, 2]               0\n",
            "          Conv2d-354            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-355            [-1, 256, 2, 2]             512\n",
            "            ReLU-356            [-1, 256, 2, 2]               0\n",
            "          Conv2d-357           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-358           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-359           [-1, 1024, 2, 2]               0\n",
            "           block-360           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-361            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-362            [-1, 256, 2, 2]             512\n",
            "            ReLU-363            [-1, 256, 2, 2]               0\n",
            "          Conv2d-364            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-365            [-1, 256, 2, 2]             512\n",
            "            ReLU-366            [-1, 256, 2, 2]               0\n",
            "          Conv2d-367           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-368           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-369           [-1, 1024, 2, 2]               0\n",
            "           block-370           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-371            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-372            [-1, 256, 2, 2]             512\n",
            "            ReLU-373            [-1, 256, 2, 2]               0\n",
            "          Conv2d-374            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-375            [-1, 256, 2, 2]             512\n",
            "            ReLU-376            [-1, 256, 2, 2]               0\n",
            "          Conv2d-377           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-378           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-379           [-1, 1024, 2, 2]               0\n",
            "           block-380           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-381            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-382            [-1, 256, 2, 2]             512\n",
            "            ReLU-383            [-1, 256, 2, 2]               0\n",
            "          Conv2d-384            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-385            [-1, 256, 2, 2]             512\n",
            "            ReLU-386            [-1, 256, 2, 2]               0\n",
            "          Conv2d-387           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-388           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-389           [-1, 1024, 2, 2]               0\n",
            "           block-390           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-391            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-392            [-1, 256, 2, 2]             512\n",
            "            ReLU-393            [-1, 256, 2, 2]               0\n",
            "          Conv2d-394            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-395            [-1, 256, 2, 2]             512\n",
            "            ReLU-396            [-1, 256, 2, 2]               0\n",
            "          Conv2d-397           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-398           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-399           [-1, 1024, 2, 2]               0\n",
            "           block-400           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-401            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-402            [-1, 256, 2, 2]             512\n",
            "            ReLU-403            [-1, 256, 2, 2]               0\n",
            "          Conv2d-404            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-405            [-1, 256, 2, 2]             512\n",
            "            ReLU-406            [-1, 256, 2, 2]               0\n",
            "          Conv2d-407           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-408           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-409           [-1, 1024, 2, 2]               0\n",
            "           block-410           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-411            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-412            [-1, 256, 2, 2]             512\n",
            "            ReLU-413            [-1, 256, 2, 2]               0\n",
            "          Conv2d-414            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-415            [-1, 256, 2, 2]             512\n",
            "            ReLU-416            [-1, 256, 2, 2]               0\n",
            "          Conv2d-417           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-418           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-419           [-1, 1024, 2, 2]               0\n",
            "           block-420           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-421            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-422            [-1, 256, 2, 2]             512\n",
            "            ReLU-423            [-1, 256, 2, 2]               0\n",
            "          Conv2d-424            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-425            [-1, 256, 2, 2]             512\n",
            "            ReLU-426            [-1, 256, 2, 2]               0\n",
            "          Conv2d-427           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-428           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-429           [-1, 1024, 2, 2]               0\n",
            "           block-430           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-431            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-432            [-1, 256, 2, 2]             512\n",
            "            ReLU-433            [-1, 256, 2, 2]               0\n",
            "          Conv2d-434            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-435            [-1, 256, 2, 2]             512\n",
            "            ReLU-436            [-1, 256, 2, 2]               0\n",
            "          Conv2d-437           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-438           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-439           [-1, 1024, 2, 2]               0\n",
            "           block-440           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-441            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-442            [-1, 256, 2, 2]             512\n",
            "            ReLU-443            [-1, 256, 2, 2]               0\n",
            "          Conv2d-444            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-445            [-1, 256, 2, 2]             512\n",
            "            ReLU-446            [-1, 256, 2, 2]               0\n",
            "          Conv2d-447           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-448           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-449           [-1, 1024, 2, 2]               0\n",
            "           block-450           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-451            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-452            [-1, 256, 2, 2]             512\n",
            "            ReLU-453            [-1, 256, 2, 2]               0\n",
            "          Conv2d-454            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-455            [-1, 256, 2, 2]             512\n",
            "            ReLU-456            [-1, 256, 2, 2]               0\n",
            "          Conv2d-457           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-458           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-459           [-1, 1024, 2, 2]               0\n",
            "           block-460           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-461            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-462            [-1, 256, 2, 2]             512\n",
            "            ReLU-463            [-1, 256, 2, 2]               0\n",
            "          Conv2d-464            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-465            [-1, 256, 2, 2]             512\n",
            "            ReLU-466            [-1, 256, 2, 2]               0\n",
            "          Conv2d-467           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-468           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-469           [-1, 1024, 2, 2]               0\n",
            "           block-470           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-471            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-472            [-1, 256, 2, 2]             512\n",
            "            ReLU-473            [-1, 256, 2, 2]               0\n",
            "          Conv2d-474            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-475            [-1, 256, 2, 2]             512\n",
            "            ReLU-476            [-1, 256, 2, 2]               0\n",
            "          Conv2d-477           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-478           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-479           [-1, 1024, 2, 2]               0\n",
            "           block-480           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-481            [-1, 512, 2, 2]         524,288\n",
            "     BatchNorm2d-482            [-1, 512, 2, 2]           1,024\n",
            "            ReLU-483            [-1, 512, 2, 2]               0\n",
            "          Conv2d-484            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-485            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-486            [-1, 512, 1, 1]               0\n",
            "          Conv2d-487           [-1, 2048, 1, 1]       1,048,576\n",
            "     BatchNorm2d-488           [-1, 2048, 1, 1]           4,096\n",
            "          Conv2d-489           [-1, 2048, 1, 1]       2,097,152\n",
            "     BatchNorm2d-490           [-1, 2048, 1, 1]           4,096\n",
            "            ReLU-491           [-1, 2048, 1, 1]               0\n",
            "           block-492           [-1, 2048, 1, 1]               0\n",
            "          Conv2d-493            [-1, 512, 1, 1]       1,048,576\n",
            "     BatchNorm2d-494            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-495            [-1, 512, 1, 1]               0\n",
            "          Conv2d-496            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-497            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-498            [-1, 512, 1, 1]               0\n",
            "          Conv2d-499           [-1, 2048, 1, 1]       1,048,576\n",
            "     BatchNorm2d-500           [-1, 2048, 1, 1]           4,096\n",
            "            ReLU-501           [-1, 2048, 1, 1]               0\n",
            "           block-502           [-1, 2048, 1, 1]               0\n",
            "          Conv2d-503            [-1, 512, 1, 1]       1,048,576\n",
            "     BatchNorm2d-504            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-505            [-1, 512, 1, 1]               0\n",
            "          Conv2d-506            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-507            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-508            [-1, 512, 1, 1]               0\n",
            "          Conv2d-509           [-1, 2048, 1, 1]       1,048,576\n",
            "     BatchNorm2d-510           [-1, 2048, 1, 1]           4,096\n",
            "            ReLU-511           [-1, 2048, 1, 1]               0\n",
            "           block-512           [-1, 2048, 1, 1]               0\n",
            "AdaptiveAvgPool2d-513           [-1, 2048, 1, 1]               0\n",
            "          Linear-514                 [-1, 1000]       2,049,000\n",
            "================================================================\n",
            "Total params: 60,192,808\n",
            "Trainable params: 60,192,808\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 12.40\n",
            "Params size (MB): 229.62\n",
            "Estimated Total Size (MB): 242.03\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Setup loss function and optimizer"
      ],
      "metadata": {
        "id": "4bxxPZil8YH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup a loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Setup an optimizer\n",
        "optimizer = optim.Adam(params=resnet18.parameters(),\n",
        "                       lr=0.01)"
      ],
      "metadata": {
        "id": "fVI8uQMjiRJJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Create training function"
      ],
      "metadata": {
        "id": "9ZYDvQzX8e4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "    EPOCHS = 15\n",
        "    train_samples_num = 45000\n",
        "    val_samples_num = 5000\n",
        "    train_costs, val_costs = [], []\n",
        "\n",
        "    #Training phase.\n",
        "    for epoch in range(EPOCHS):\n",
        "\n",
        "        train_running_loss = 0\n",
        "        correct_train = 0\n",
        "\n",
        "        resnet18.train().cuda()\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Start the forward pass\n",
        "            prediction = resnet18(inputs)\n",
        "\n",
        "            loss = criterion(prediction, labels)\n",
        "\n",
        "            # do backpropagation and update weights with step()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print('outputs on which to apply torch.max ', prediction)\n",
        "            # find the maximum along the rows, use dim=1 to torch.max()\n",
        "            _, predicted_outputs = torch.max(prediction.data, 1)\n",
        "\n",
        "            # Update the running corrects\n",
        "            correct_train += (predicted_outputs == labels).float().sum().item()\n",
        "\n",
        "            train_running_loss += (loss.data.item() * inputs.shape[0])\n",
        "\n",
        "\n",
        "        train_epoch_loss = train_running_loss / train_samples_num\n",
        "\n",
        "        train_costs.append(train_epoch_loss)\n",
        "\n",
        "        train_acc =  (correct_train / train_samples_num) * 100\n",
        "\n",
        "        # Now check trained weights on the validation set\n",
        "        val_running_loss = 0\n",
        "        correct_val = 0\n",
        "\n",
        "        resnet18.eval().cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                # Forward pass.\n",
        "                prediction = resnet18(inputs)\n",
        "\n",
        "                # Compute the loss.\n",
        "                loss = criterion(prediction, labels)\n",
        "\n",
        "                # Compute validation accuracy.\n",
        "                _, predicted_outputs = torch.max(prediction.data, 1)\n",
        "                correct_val += (predicted_outputs == labels).float().sum().item()\n",
        "\n",
        "            # Compute batch loss.\n",
        "            val_running_loss += (loss.data.item() * inputs.shape[0])\n",
        "\n",
        "            val_epoch_loss = val_running_loss / val_samples_num\n",
        "            val_costs.append(val_epoch_loss)\n",
        "            val_acc = ( correct_val / val_samples_num) * 100\n",
        "\n",
        "        info = \"[Epoch {}/{}]: train-loss = {:0.6f} | train-acc = {:0.3f} | val-loss = {:0.6f} | val-acc = {:0.3f}\"\n",
        "\n",
        "        print(info.format(epoch+1, EPOCHS, train_epoch_loss, train_acc, val_epoch_loss, val_acc))\n",
        "\n",
        "        torch.save(resnet18.state_dict(), '/content/checkpoint_gpu_{}'.format(epoch + 1))\n",
        "\n",
        "    torch.save(resnet18.state_dict(), '/content/resnet-18_weights_gpu')\n",
        "\n",
        "    return train_costs, val_costs"
      ],
      "metadata": {
        "id": "yiUY_dKDiaKB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_costs, val_costs = train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQtmXNUlinOi",
        "outputId": "64db34f4-d281-4ffa-daf4-95194318d7bb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/15]: train-loss = 2.049946 | train-acc = 32.271 | val-loss = 0.002708 | val-acc = 42.020\n",
            "[Epoch 2/15]: train-loss = 1.521984 | train-acc = 44.787 | val-loss = 0.001357 | val-acc = 46.980\n",
            "[Epoch 3/15]: train-loss = 1.345652 | train-acc = 51.596 | val-loss = 0.002675 | val-acc = 51.700\n",
            "[Epoch 4/15]: train-loss = 1.207609 | train-acc = 57.016 | val-loss = 0.001865 | val-acc = 58.140\n",
            "[Epoch 5/15]: train-loss = 1.062926 | train-acc = 62.627 | val-loss = 0.001352 | val-acc = 63.960\n",
            "[Epoch 6/15]: train-loss = 0.959758 | train-acc = 66.600 | val-loss = 0.000669 | val-acc = 65.760\n",
            "[Epoch 7/15]: train-loss = 0.871462 | train-acc = 69.816 | val-loss = 0.003585 | val-acc = 65.060\n",
            "[Epoch 8/15]: train-loss = 0.790303 | train-acc = 72.456 | val-loss = 0.000972 | val-acc = 69.160\n",
            "[Epoch 9/15]: train-loss = 0.704673 | train-acc = 75.542 | val-loss = 0.001403 | val-acc = 71.660\n",
            "[Epoch 10/15]: train-loss = 0.648904 | train-acc = 77.633 | val-loss = 0.001959 | val-acc = 73.100\n",
            "[Epoch 11/15]: train-loss = 0.585838 | train-acc = 80.016 | val-loss = 0.000692 | val-acc = 73.880\n",
            "[Epoch 12/15]: train-loss = 0.506157 | train-acc = 82.464 | val-loss = 0.000341 | val-acc = 73.660\n",
            "[Epoch 13/15]: train-loss = 0.450323 | train-acc = 84.304 | val-loss = 0.000837 | val-acc = 74.720\n",
            "[Epoch 14/15]: train-loss = 0.411529 | train-acc = 85.856 | val-loss = 0.002568 | val-acc = 74.460\n",
            "[Epoch 15/15]: train-loss = 0.349689 | train-acc = 87.938 | val-loss = 0.000865 | val-acc = 74.020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet18 = ResNet18()\n",
        "resnet18.load_state_dict(torch.load('/content/resnet-18_weights_gpu'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7oI1zZDirK0",
        "outputId": "75bedafb-3a70-4bae-bc68-df08fa3deeac"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Creating testing function"
      ],
      "metadata": {
        "id": "b-VXoR0N9DIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_samples_num = 10000\n",
        "correct = 0\n",
        "\n",
        "resnet18.eval().cuda()\n",
        "\n",
        "with  torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # Make predictions.\n",
        "        prediction = resnet18(inputs)\n",
        "\n",
        "        # Retrieve predictions indexes.\n",
        "        _, predicted_class = torch.max(prediction.data, 1)\n",
        "\n",
        "        # Compute number of correct predictions.\n",
        "        correct += (predicted_class == labels).float().sum().item()\n",
        "\n",
        "test_accuracy = (correct / test_samples_num) * 100\n",
        "print('Test accuracy: {}%'.format(test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f8pAsWrmuNy",
        "outputId": "80f8e0a1-2b23-4c68-f030-92fb289e88c1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 74.35000000000001%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J-3JRGsn9nPK"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}