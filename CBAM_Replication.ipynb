{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKqGSzfZUtJ0Eo2Ir04W7n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhardwajArjit/Research-Paper-Replication/blob/main/CBAM_Replication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This notebook replicates the research paper titled \"**CBAM: Convolutional Block Attention Model**\" with PyTorch.\n",
        "\n",
        "The link to paper: https://arxiv.org/abs/1807.06521\n",
        "\n",
        "CBAM (Convolutional Block Attention Module) aims to enhance the feature representation of convolutional neural networks by incorporating channel-wise and spatial-wise attention mechanisms."
      ],
      "metadata": {
        "id": "c6cvCobcLuno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Get setup"
      ],
      "metadata": {
        "id": "jK8zEfVtNuAP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUbeBCfmJkbu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Channel Attention Module"
      ],
      "metadata": {
        "id": "UK1m83IfN8mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class channel_attention_module(nn.Module):\n",
        "  def __init__(self, ch, ratio=8):\n",
        "    super().__init__()\n",
        "\n",
        "    self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "    self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(ch, ch // ratio, bias=False),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(ch // ratio, ch, bias=False)\n",
        "    )\n",
        "\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_1 = self.avg_pool(x).squeeze(-1).squeeze(-1)\n",
        "    x_1 = self.mlp(x_1)\n",
        "    x_2 = self.max_pool(x).squeeze(-1).squeeze(-1)\n",
        "    x_2 = self.mlp(x_2)\n",
        "\n",
        "    feats = x_1 + x_2\n",
        "    feats = self.sigmoid(feats).unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "    refined_features = x * feats\n",
        "\n",
        "    return refined_features\n",
        "\n",
        "    print(x_1.shape, x_2.shape)"
      ],
      "metadata": {
        "id": "JaOysCB3KtON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the results of channel attention module\n",
        "x = torch.randn((8, 32, 128, 128))\n",
        "module_1 = channel_attention_module(32)\n",
        "y = module_1(x)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yO07Z6jLsOK",
        "outputId": "45cdf497-820e-4ca5-ce5c-922f0aa695e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 32, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Spatial Attention Module"
      ],
      "metadata": {
        "id": "pcje7gcKOREq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class spatial_attention_module(nn.Module):\n",
        "  def __init__(self, kernel_size=7):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv = nn.Conv2d(2, 1, kernel_size, padding=3, bias=False)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_1 = torch.mean(x, dim=1, keepdim=True)\n",
        "    x_2, _ = torch.max(x, dim=1, keepdim=True)\n",
        "\n",
        "    feats = torch.cat([x_1, x_2], dim=1)\n",
        "    feats = self.conv(feats)\n",
        "    feats = self.sigmoid(feats)\n",
        "\n",
        "    refined_features = x * feats\n",
        "    return refined_features"
      ],
      "metadata": {
        "id": "uvpFVLNRL9NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the results of spatial attention module\n",
        "x = torch.randn((8, 32, 128, 128))\n",
        "module_2 = spatial_attention_module()\n",
        "y = module_2(x)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iywgocUYN1MN",
        "outputId": "339bdd90-684d-4dd8-ea6c-f2ed3fa91f6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 32, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. CBAM (Convolutional Block Attention Module)"
      ],
      "metadata": {
        "id": "3mrnHH8COcdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CBAM(nn.Module):\n",
        "  def __init__(self, channel):\n",
        "    super().__init__()\n",
        "\n",
        "    self.channel_layer = channel_attention_module(channel)\n",
        "    self.spatial_layer = spatial_attention_module()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.channel_layer(x)\n",
        "    x = self.spatial_layer(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "-w0Ku4xVN3_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the results of cbam\n",
        "x = torch.randn((8, 32, 128, 128))\n",
        "model_1 = CBAM(32)\n",
        "y = model_1(x)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzoxDY9vO-wC",
        "outputId": "378cb453-87fe-4cf3-a209-f982a10c9ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 32, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DUYuElbyPIku"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}