{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtahHb4o3v8KgocSjBq0lO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhardwajArjit/Research-Paper-Replication/blob/main/CBAM_Replication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This notebook replicates the research paper titled \"**CBAM: Convolutional Block Attention Module**\" with PyTorch.\n",
        "\n",
        "The link to paper: https://arxiv.org/abs/1807.06521\n",
        "\n",
        "CBAM (Convolutional Block Attention Module) aims to enhance the feature representation of convolutional neural networks by incorporating channel-wise and spatial-wise attention mechanisms.\n",
        "\n",
        "The channel module focuses on \"what\" is meaningful in the given input image whereas the spatial module focuses on \"where\" the meaningful features are in the image."
      ],
      "metadata": {
        "id": "c6cvCobcLuno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Get setup"
      ],
      "metadata": {
        "id": "jK8zEfVtNuAP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PUbeBCfmJkbu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Channel Attention Module"
      ],
      "metadata": {
        "id": "UK1m83IfN8mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class channel_attention_module(nn.Module):\n",
        "  def __init__(self, channel, ratio=8):\n",
        "    super().__init__()\n",
        "\n",
        "    # defining average pooling layer\n",
        "    self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "    # defining max pooling layer\n",
        "    self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "    # defining multi-layer perceptron\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(in_features=channel,\n",
        "                  out_features=channel // ratio,\n",
        "                  bias=False),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(in_features=channel // ratio,\n",
        "                  out_features=channel,\n",
        "                  bias=False)\n",
        "    )\n",
        "    # defining the sigmoid function\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x1 = self.avg_pool(x).squeeze(-1).squeeze(-1)\n",
        "    x1 = self.mlp(x1)\n",
        "    x2 = self.max_pool(x).squeeze(-1).squeeze(-1)\n",
        "    x2 = self.mlp(x2)\n",
        "\n",
        "    feats = x1 + x2\n",
        "    feats = self.sigmoid(feats).unsqueeze(-1).unsqueeze(-1)\n",
        "    # multiplying the output of channel attention module with input features\n",
        "    refined_features = x * feats\n",
        "\n",
        "    return refined_features"
      ],
      "metadata": {
        "id": "JaOysCB3KtON"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the results of channel attention module\n",
        "x = torch.randn((8, 32, 128, 128))\n",
        "module_1 = channel_attention_module(32)\n",
        "y = module_1(x)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yO07Z6jLsOK",
        "outputId": "713e02f7-ff6a-46b6-e7f8-37baf90d1aec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 32, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Spatial Attention Module"
      ],
      "metadata": {
        "id": "pcje7gcKOREq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class spatial_attention_module(nn.Module):\n",
        "  def __init__(self, kernel_size=7):\n",
        "    super().__init__()\n",
        "\n",
        "    # defining the convolutional layer\n",
        "    self.conv = nn.Conv2d(in_channels=2,\n",
        "                          out_channels=1,\n",
        "                          kernel_size=kernel_size,\n",
        "                          padding=3,\n",
        "                          bias=False)\n",
        "    # defining the sigmoid function\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x1 = torch.mean(x, dim=1, keepdim=True)\n",
        "    x2, _ = torch.max(x, dim=1, keepdim=True)\n",
        "    # concatenate x1 and x2 to generate more efficient feature descriptor\n",
        "    feats = torch.cat([x1, x2], dim=1)\n",
        "    # passing the features to compute 2d spatial attention map\n",
        "    feats = self.conv(feats)\n",
        "    feats = self.sigmoid(feats)\n",
        "\n",
        "    refined_features = x * feats\n",
        "    return refined_features"
      ],
      "metadata": {
        "id": "uvpFVLNRL9NZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the results of spatial attention module\n",
        "x = torch.randn((8, 32, 128, 128))\n",
        "module_2 = spatial_attention_module()\n",
        "y = module_2(x)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iywgocUYN1MN",
        "outputId": "4f146774-f526-4a68-aca0-ae401a91f11e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 32, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. CBAM (Convolutional Block Attention Module)"
      ],
      "metadata": {
        "id": "3mrnHH8COcdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CBAM(nn.Module):\n",
        "  def __init__(self, channel):\n",
        "    super().__init__()\n",
        "\n",
        "    self.channel_layer = channel_attention_module(channel)\n",
        "    self.spatial_layer = spatial_attention_module()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # the channel and spatial modules are arranged in parallel manner\n",
        "    x = self.channel_layer(x)\n",
        "    x = self.spatial_layer(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "-w0Ku4xVN3_v"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the results of cbam\n",
        "x = torch.randn((8, 32, 128, 128))\n",
        "cbam = CBAM(32)\n",
        "y = cbam(x)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzoxDY9vO-wC",
        "outputId": "4951c1d2-7331-4002-8ecf-b57288bdaf7e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 32, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DUYuElbyPIku"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}