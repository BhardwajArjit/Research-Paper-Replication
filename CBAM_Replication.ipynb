{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFEDuiXMD8Q2TFMKMruwrO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhardwajArjit/Research-Paper-Replication/blob/main/CBAM_Replication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This notebook replicates the research paper titled \"**CBAM: Convolutional Block Attention Module**\" with PyTorch.\n",
        "\n",
        "The link to paper: https://arxiv.org/abs/1807.06521\n",
        "\n",
        "CBAM (Convolutional Block Attention Module) aims to enhance the feature representation of convolutional neural networks by incorporating channel-wise and spatial-wise attention mechanisms.\n",
        "\n",
        "The channel module focuses on \"what\" is meaningful in the given input image whereas the spatial module focuses on \"where\" the meaningful features are in the image."
      ],
      "metadata": {
        "id": "c6cvCobcLuno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Get setup"
      ],
      "metadata": {
        "id": "jK8zEfVtNuAP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PUbeBCfmJkbu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Channel Attention Module"
      ],
      "metadata": {
        "id": "UK1m83IfN8mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class channel_attention_module(nn.Module):\n",
        "  \"\"\"\n",
        "  Channel Attention Module.\n",
        "\n",
        "  Parameters:\n",
        "  - channel (int): Number of input channels.\n",
        "  - ratio (int, optional): Reduction ratio for the intermediate channels. Default is 8.\n",
        "  \"\"\"\n",
        "  def __init__(self, channel, ratio=8):\n",
        "    super().__init__()\n",
        "\n",
        "    # Adaptive average pooling layer to capture global average information\n",
        "    self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "    # Adaptive max pooling layer to capture global max information\n",
        "    self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "    # Multi-layer perceptron (MLP) for modeling channel dependencies\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(in_features=channel,\n",
        "                  out_features=channel // ratio,\n",
        "                  bias=False),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(in_features=channel // ratio,\n",
        "                  out_features=channel,\n",
        "                  bias=False)\n",
        "    )\n",
        "    # Sigmoid activation function to produce attention weights between 0 and 1\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x1 = self.avg_pool(x).squeeze(-1).squeeze(-1)\n",
        "    x1 = self.mlp(x1)\n",
        "    x2 = self.max_pool(x).squeeze(-1).squeeze(-1)\n",
        "    x2 = self.mlp(x2)\n",
        "    # Element-wise addition of the MLP-transformed average and max pooling results\n",
        "    feats = x1 + x2\n",
        "    # Apply sigmoid activation to produce attention weights, reshape to (batch_size, channels, 1, 1)\n",
        "    feats = self.sigmoid(feats).unsqueeze(-1).unsqueeze(-1)\n",
        "    # Multiply the output of the channel attention module with input features\n",
        "    refined_features = x * feats\n",
        "\n",
        "    return refined_features"
      ],
      "metadata": {
        "id": "JaOysCB3KtON"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the results of channel attention module\n",
        "x = torch.randn((8, 32, 128, 128))\n",
        "module_1 = channel_attention_module(32)\n",
        "y = module_1(x)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yO07Z6jLsOK",
        "outputId": "e3e9780d-4cd8-4393-e5a0-077a6f30d823"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 32, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Spatial Attention Module"
      ],
      "metadata": {
        "id": "pcje7gcKOREq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class spatial_attention_module(nn.Module):\n",
        "  \"\"\"\n",
        "  Spatial Attention Module.\n",
        "\n",
        "  Parameters:\n",
        "  - kernel_size (int, optional): Size of the convolutional kernel. Default is 7.\n",
        "  \"\"\"\n",
        "  def __init__(self, kernel_size=7):\n",
        "    super().__init__()\n",
        "\n",
        "    # Convolutional layer for generating spatial attention map\n",
        "    self.conv = nn.Conv2d(in_channels=2,\n",
        "                          out_channels=1,\n",
        "                          kernel_size=kernel_size,\n",
        "                          padding=3,\n",
        "                          bias=False)\n",
        "    # Sigmoid activation function to produce attention weights between 0 and 1\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Compute the mean along the channel dimension\n",
        "    x1 = torch.mean(x, dim=1, keepdim=True)\n",
        "    # Compute the maximum along the channel dimension\n",
        "    x2, _ = torch.max(x, dim=1, keepdim=True)\n",
        "    # concatenate x1 and x2 to generate more efficient feature descriptor\n",
        "    feats = torch.cat([x1, x2], dim=1)\n",
        "    # Pass the concatenated features through the convolutional layer to compute the 2D spatial attention map\n",
        "    feats = self.conv(feats)\n",
        "    # Apply sigmoid activation to produce attention weights\n",
        "    feats = self.sigmoid(feats)\n",
        "    # Multiply the output of the spatial attention module with input features\n",
        "    refined_features = x * feats\n",
        "    return refined_features"
      ],
      "metadata": {
        "id": "uvpFVLNRL9NZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the results of spatial attention module\n",
        "x = torch.randn((8, 32, 128, 128))\n",
        "module_2 = spatial_attention_module()\n",
        "y = module_2(x)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iywgocUYN1MN",
        "outputId": "01590760-5f04-41a1-f7f3-2179dc7699cf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 32, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. CBAM (Convolutional Block Attention Module)"
      ],
      "metadata": {
        "id": "3mrnHH8COcdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CBAM(nn.Module):\n",
        "  \"\"\"\n",
        "  Convolutional Block Attention Module (CBAM).\n",
        "\n",
        "  Parameters:\n",
        "  - channel (int): Number of input channels.\n",
        "  \"\"\"\n",
        "  def __init__(self, channel):\n",
        "    super().__init__()\n",
        "    # Channel Attention Module\n",
        "    self.channel_layer = channel_attention_module(channel)\n",
        "    # Spatial Attention Module\n",
        "    self.spatial_layer = spatial_attention_module()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # the channel and spatial modules are arranged in parallel manner\n",
        "    x = self.channel_layer(x)\n",
        "    x = self.spatial_layer(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "-w0Ku4xVN3_v"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the results of cbam\n",
        "x = torch.randn((8, 32, 128, 128))\n",
        "cbam = CBAM(32)\n",
        "y = cbam(x)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzoxDY9vO-wC",
        "outputId": "4973c6d7-4415-4432-e8bb-32a13688eb9e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 32, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DUYuElbyPIku"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}